[
  {
    "authors": [
      "Xiaoran Xu",
      "Songpeng Zu",
      "Chengliang Gao",
      "Yuan Zhang",
      "Wei Feng"
    ],
    "categories": null,
    "content": "",
    "date": 1541062794,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541062794,
    "objectID": "1f1f2d6d5b248b30f9beb9580bc61712",
    "permalink": "https://www.xiaoranxu.com/publication/modeling-attflow/",
    "publishdate": "2018-11-01T16:59:54+08:00",
    "relpermalink": "/publication/modeling-attflow/",
    "section": "publication",
    "summary": "Real-world scenarios demand reasoning about process, more than final outcome prediction, to discover latent causal chains and better understand complex systems. It requires the learning algorithms to offer both accurate predictions and clear interpretations. We design a set of trajectory reasoning tasks on graphs with only the source and the destination observed. We present the attention flow mechanism to explicitly model the reasoning process, leveraging the relational inductive biases by basing our models on graph networks. We study the way attention flow can effectively act on the underlying information flow implemented by message passing. Experiments demonstrate that the attention flow driven by and interacting with graph networks can provide higher accuracy in prediction and better interpretation for trajectories reasoning.",
    "tags": [
      "Attention Flow",
      "Causal Chains",
      "Graph Networks",
      "Random Walk",
      "Reasoning"
    ],
    "title": "Modeling Attention Flow on Graphs",
    "type": "publication"
  },
  {
    "authors": [
      "Xiaoran Xu",
      "Laming Chen",
      "Songpeng Zu",
      "Hanning Zhou"
    ],
    "categories": null,
    "content": "",
    "date": 1537977600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1537977600,
    "objectID": "1d5e0e80307304e62024a295ab2bb3e6",
    "permalink": "https://www.xiaoranxu.com/publication/hulu-video-reco/",
    "publishdate": "2018-09-27T00:00:00+08:00",
    "relpermalink": "/publication/hulu-video-reco/",
    "section": "publication",
    "summary": "Online Video Streaming services such as Hulu hosts tens of millions of premium videos, which requires an effective recommendation system to help viewers discover what they enjoy. In this talk, we will introduce Hulu's recent technical progresses in recommender systems and deep-dive into the topic of generating recommendation reason from knowledge graph. We have two user scenarios: the store-shelf and autoplay. The first requires a list of videos to maximize the chance that a viewer would pick one of them to watch. The second requires a sequence of video recommendations such that the viewer would continuously watch within the current session.\n\nIn the model layer, we designed specific models to match with each user scenario, balancing both exploitation and exploration. For example, we leverage the contextual-bandit model in the store-shelf scenario to adapt the ranking strategy to various types of user feedbacks. To optimize exploitation, we tested several granularity levels for parameter sharing among the arms. For more effective exploration, we incorporate Thomason sampling. For the autoplay scenario, we use a contextual recurrent neural network to predict the next video that the viewer is going to watch.\n\nIn the feature and data layer, we train embeddings for content, user and contextual info. For example, to train content embeddings, we collect factual tags from metadata, sentiment tags from reviews, and keywords from the captions and object/action recognized using computer vision techniques.\n\nNext we will deep-dive into one important topic: generating recommendation reason from knowledge graph.\n\nA fact is defined by a tuple of related entities and their relation, which is normally a pair of entities tagged by a relationship. In our problem setting, recommendation results are the targets, viewed as inputs for the reasoning task, consisting of pairs of relevant entities, i.e. a source node and a destination node in a knowledge graph. The recommendation reasoning task is to learn a path or a small directed acyclic subgraph, connecting the source node to the destination node.\n\nSince the facts in a knowledge graph have different confidence values for different reasoned targets, we need to conduct a probabilistic inference. The challenge is we do not know a predefined set of logic rules to guide the search through the knowledge graph, which prevents us from directly applying the probabilistic logic methods. Inspired by recent advances in deep learning and reinforcement learning, especially in graph neural networks, attention mechanism and deep generative models, we propose two ways to model the reasoning process: the differentiable reasoning approach and the stochastic reasoning approach.\n\nDifferentiable reasoning approaches are based on graph neural networks [1,2] with attention flow and information flow. The attention dynamics is an iterative process of redistributing and aggregating attention over the knowledge graph, starting at the source node. The final attention aggregated at the destination node serves for the prediction to compute the loss. Instead of the prediction accuracy, we care more about how the learned attention dynamics draws its reasoning track in a knowledge graph.\n\nStochastic reasoning approaches frame the reasoning process as learning a probabilistic graphical model consisting of stochastic discrete operations, such as selecting a node and selecting an edge, to build a reason subgraph extracted from the knowledge graph. The model is known as stochastic computation graphs (SCGs), and to learn it, we propose a generalized back-propagation framework Backprop-Q [3] to overcome the gradient-blocking issues in applying standard back-propagation. In summary, we give an overview of the recommendation research in Hulu and deep-dive into our differentiable reasoning approach and stochastic reasoning approach for generating recommendation reasons based on a knowledge graph.",
    "tags": [],
    "title": "Hulu video recommendation: from relevance to reasoning",
    "type": "publication"
  },
  {
    "authors": null,
    "categories": null,
    "content": " In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n",
    "date": 1536422400,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1536422400,
    "objectID": "6a451186c775f5f0adb3a0416d0cb711",
    "permalink": "https://www.xiaoranxu.com/tutorial/example/",
    "publishdate": "2018-09-09T00:00:00+08:00",
    "relpermalink": "/tutorial/example/",
    "section": "tutorial",
    "summary": "In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;",
    "tags": null,
    "title": "Example Page",
    "type": "docs"
  },
  {
    "authors": [
      "Yuan Zhang",
      "Xiaoran Xu",
      "Hanning Zhou",
      "Yan Zhang"
    ],
    "categories": null,
    "content": "",
    "date": 1535299200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1535299200,
    "objectID": "a99eff11fa30c50fb00719f332eb2c12",
    "permalink": "https://www.xiaoranxu.com/publication/distilling-structured-knowledge/",
    "publishdate": "2018-08-27T00:00:00+08:00",
    "relpermalink": "/publication/distilling-structured-knowledge/",
    "section": "publication",
    "summary": "Recently, the embedding-based recommendation models (e.g., matrix factorization and deep models) are prevalent in both academia and industry due to their effectiveness and flexibility. However, they also have such intrinsic limitations as lacking explainability and suffering from data sparsity. In this paper, we propose an end-to-end joint learning framework to get around these limitations without introducing any extra overhead by distilling structured knowledge from a differentiable path-based recommendation model. Through extensive experiments, we show that our proposed framework can achieve state-of-the-art recommendation performance and meanwhile provide well interpretable recommendation reasons.",
    "tags": [
      "Explainable Recommendation",
      "Knowledge Distillation",
      "Differentiable Path-based Model"
    ],
    "title": "Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendation",
    "type": "publication"
  },
  {
    "authors": [
      "Xiaoran Xu",
      "Songpeng Zu",
      "Yuan Zhang",
      "Hanning Zhou",
      "Wei Feng"
    ],
    "categories": null,
    "content": "",
    "date": 1532484384,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1532484384,
    "objectID": "54f07db9336d701e66d9f1c1695cb427",
    "permalink": "https://www.xiaoranxu.com/publication/backprop-q/",
    "publishdate": "2018-07-25T10:06:24+08:00",
    "relpermalink": "/publication/backprop-q/",
    "section": "publication",
    "summary": "In real-world scenarios, it is appealing to learn a model carrying out stochastic operations internally, known as stochastic computation graphs (SCGs), rather than learning a deterministic mapping. However, standard backpropagation is not applicable to SCGs. We attempt to address this issue from the angle of cost propagation, with local surrogate costs, called Q-functions, constructed and learned for each stochastic node in an SCG. Then, the SCG can be trained based on these surrogate costs using standard backpropagation. We propose the entire framework as a solution to generalize backpropagation for SCGs, which resembles an actor-critic architecture but based on a graph. For broad applicability, we study a variety of SCG structures from one cost to multiple costs. We utilize recent advances in reinforcement learning (RL) and variational Bayes (VB), such as off-policy critic learning and unbiased-and-low-variance gradient estimation, and review them in the context of SCGs. The generalized backpropagation extends transported learning signals beyond gradients between stochastic nodes while preserving the benefit of backpropagating gradients through deterministic nodes. Experimental suggestions and concerns are listed to help design and test any specific model using this framework.",
    "tags": [
      "Backpropagation",
      "Credit Assignment",
      "Stochastic Computation Graphs"
    ],
    "title": "Backprop-Q: Generalized Backpropagation for Stochastic Computation Graphs",
    "type": "publication"
  },
  {
    "authors": null,
    "categories": null,
    "content": " Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n",
    "date": -62135596800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": -62135596800,
    "objectID": "c2915ec5da95791851caafdcba9664af",
    "permalink": "https://www.xiaoranxu.com/slides/example-slides/",
    "publishdate": "0001-01-01T00:00:00Z",
    "relpermalink": "/slides/example-slides/",
    "section": "slides",
    "summary": "Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$",
    "tags": null,
    "title": "Slides",
    "type": "slides"
  }
]